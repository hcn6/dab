{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis + Back translation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vietai/dab/blob/master/colab/Sentiment_Analysis_%2B_Back_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1h6HzC9GopY",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this colab, we walk you through how to use our code in `vietai/dab` to augment an example of a Vietnamese dataset. The dataset is relatively small: it consists of only 27,000 supervised examples in its training set - perfect for some experiments on data augmentation!\n",
        "\n",
        "For this task, we have already trained vi --> en and en --> vi translation models and placed them on Google Cloud Storage. There is no need to train new ones unless you have more data or data that fits the downstream task  better -- more on this later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc6FUSgiVLpV",
        "colab_type": "text"
      },
      "source": [
        "**MIT License**\n",
        "\n",
        "Copyright (c) [2019] [Trieu H. Trinh](https://thtrieu.github.io/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwAofVf_Ha4J",
        "colab_type": "text"
      },
      "source": [
        "# Install dependencies.\n",
        "\n",
        "* `gcsfuse` to mount this colab to the Google Cloud Storage. Alternatively you can also mount the colab to Google Drive if you do not have access to GCS.\n",
        "* `tensor2tensor`: a library with all necessary tools to perform training/inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWc1zBeFfwtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount the bucket to colab, so that python package os can access to it.\n",
        "# First we install gcsfuse to be able to mount Google Cloud Storage with Colab.\n",
        "print('\\nInstalling gcsfuse')\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "# Now we install tensor2tensor\n",
        "!pip install -q -U tensor2tensor\n",
        "print('All done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fTbd76gAp4S",
        "colab_type": "text"
      },
      "source": [
        "# (Maybe) Connect to 8 TPUs and mount to Cloud Storage.\n",
        "\n",
        "You will be asked to authenticate your google account (gmail) through the browser. This is needed to have access to storages (google cloud/ google drive). Otherwise, the default Colab local storage will be used. This storage is wiped the moment you refresh the browser/ restart the runtime - which is not good for tasks that require long running and checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ighlWX6c4Ex0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pprint\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# In case of TPU runtime, we need to authenticate with google cloud\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  use_tpu = True\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    # Upload credentials to TPU.\n",
        "\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "else:\n",
        "  use_tpu, tpu_address = False, ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIz_XIK7MPz6",
        "colab_type": "text"
      },
      "source": [
        "Now we mount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZg0L8Ldg9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we mount the local storage to the google cloud bucket.\n",
        "bucket = 'vien-translation'\n",
        "print('Mounting bucket {} to local.'.format(bucket))\n",
        "mount_point = '/content/{}'.format(bucket)\n",
        "\n",
        "if not os.path.exists(mount_point):\n",
        "  tf.gfile.MakeDirs(mount_point)\n",
        "\n",
        "!fusermount -u $mount_point\n",
        "!gcsfuse --implicit-dirs $bucket $mount_point\n",
        "!ls $mount_point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bK-PsYZKIbz",
        "colab_type": "text"
      },
      "source": [
        "# Download Foody Sentiment Analysis dataset\n",
        "\n",
        "Here we use data crawled from Foody.vn with 30,000 annotated examples by `streetcodevn.com`, with a half-half ratio of negative to positive examples. This dataset is used as an assignment in our VietAI Foundation class.\n",
        "\n",
        "We split this dataset into 27,000 examples for training and the remaining 3,000 examples for testing. Use the following script to download the data. After downloading you will see `train.csv` and `test.csv`, which correspond to the train and test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-J3ZW9-KAiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "foody_dir = os.path.join(mount_point, 'foody_sa')\n",
        "gdd.download_file_from_google_drive(file_id='1bBcVHVPtXbzgwpkDYXC4zfTI8mek4Xsu', \n",
        "                                    dest_path='{}/assignment4-data.zip'.format(foody_dir), \n",
        "                                    unzip=True)\n",
        "foody_data_dir = os.path.join(foody_dir, 'data')\n",
        "!ls $foody_data_dir/t*.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkF1BTs5GC_U",
        "colab_type": "text"
      },
      "source": [
        "# Examine the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yMoUqVGDYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_csv = os.path.join(foody_data_dir, 'train.csv')\n",
        "train_df = pd.read_csv(train_csv)\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOV_eCnJKRNj",
        "colab_type": "text"
      },
      "source": [
        "# Extract texts\n",
        "\n",
        "We extract the text-only part of our dataset and examine them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g88MYf6UKP1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = list(train_df['text'])\n",
        "\n",
        "for text in all_texts[:10]:\n",
        "  print(text + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBpbK-EHLpTx",
        "colab_type": "text"
      },
      "source": [
        "This in comparison with data for our translation task (IWSLT'15) is in fact drastically different! \n",
        "\n",
        "To see that, let's examine the test data from IWSLT'15:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWUrgFWlLwln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 32 $mount_point/raw/tst2013.vi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsi5Ma09Mi6w",
        "colab_type": "text"
      },
      "source": [
        "What you are seeing here is called \"domain mismatch\", where the distribution of the two datasets are (wildly) different. In the reviews, you might see:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "* Idioms \"Cá lớn nuốt cá bé\" \"Hãy buông đôi tay nhau ra\" \"Không yêu đừng nói lời cay đắng\"\n",
        "* New words \"Gấu mình dẫn đi ăn. \"\n",
        "* Short-hand writing \"đồ ăn ko dc ngon lm đâu\"\n",
        "* Teen code \"HjHj ch0 n4y 4n k dc ngon l4'm :3:3\"\n",
        "* Capitalization \"TÔI KHÔNG THÍCH ĂN QUÁN NÀY cHúT nàO!!\"\n",
        "* Tone marks \"toi ko thich an quan nay chut nao!!\"\n",
        "* Grammatical error.\n",
        "```\n",
        "\n",
        "While you can hardly find such instances in the translation training data IWSLT'15. Expectedly the translation models won't be able to understand such instances and give low quality translation.\n",
        "\n",
        "To somewhat bridge this gap in domain mismatch, let's perform some preprocessing to our Foody Sentiment Analysis dataset. This should include special character filtering, lower-casing, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZOtcAbJLKOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removes parentheses etc., and leaves only alphanumeric characters\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 .,:!?]+\")\n",
        "\n",
        "def clean_sentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())\n",
        "\n",
        "\n",
        "def back_translate_preprocess(text):\n",
        "  # We only use the first 32 words on each line:\n",
        "  lines = []\n",
        "  for line in text.split('\\n'):\n",
        "    line = ' '.join(line.split()[:32])\n",
        "    line = clean_sentences(line)\n",
        "    lines.append(line)\n",
        "  text = '\\n'.join(lines)\n",
        "  \n",
        "  # Replace the _ character with a proper space.\n",
        "  text = text.replace('_', ' ')\n",
        "  text = text.strip()\n",
        "\n",
        "  # Since there is a lot of upper-casing in reviews,\n",
        "  # e.g. QUÁN NÀY LÀM ĂN QUÁ TỆ, ĐỒ ĂN KHÔNG SẠCH SẼ!!!\n",
        "  # We might need to lower-case all the text.\n",
        "  text = text.replace('Đ', 'đ')\n",
        "\n",
        "  text = text.replace('E', 'e')\n",
        "  text = text.replace('È', 'è')\n",
        "  text = text.replace('É', 'é')\n",
        "  text = text.replace('Ẻ', 'ẻ')\n",
        "  text = text.replace('Ẽ', 'ẽ')\n",
        "  text = text.replace('Ẹ', 'ẹ')\n",
        "\n",
        "  text = text.replace('Ê', 'ê')\n",
        "  text = text.replace('Ề', 'ề')\n",
        "  text = text.replace('Ế', 'ế')\n",
        "  text = text.replace('Ể', 'ể')\n",
        "  text = text.replace('Ễ', 'ễ')\n",
        "  text = text.replace('Ệ', 'ệ')\n",
        "\n",
        "  text = text.replace('A', 'a')\n",
        "  text = text.replace('À', 'à')\n",
        "  text = text.replace('Á', 'á')\n",
        "  text = text.replace('Ả', 'ả')\n",
        "  text = text.replace('Ã', 'ã')\n",
        "  text = text.replace('Ạ', 'ạ')\n",
        "\n",
        "  text = text.replace('Ă', 'ă')\n",
        "  text = text.replace('Ằ', 'ằ')\n",
        "  text = text.replace('Ắ', 'ắ')\n",
        "  text = text.replace('Ẳ', 'ẳ')\n",
        "  text = text.replace('Ẵ', 'ẵ')\n",
        "  text = text.replace('Ặ', 'ặ')\n",
        "\n",
        "  text = text.replace('Â', 'â')\n",
        "  text = text.replace('Ầ', 'ầ')\n",
        "  text = text.replace('Ấ', 'ấ')\n",
        "  text = text.replace('Ẫ', 'ẫ')\n",
        "  text = text.replace('Ẩ', 'ẩ')\n",
        "  text = text.replace('Ậ', 'ậ')\n",
        "\n",
        "  text = text.replace('U', 'u')\n",
        "  text = text.replace('Ù', 'ù')\n",
        "  text = text.replace('Ú', 'ú')\n",
        "  text = text.replace('Ủ', 'ủ')\n",
        "  text = text.replace('Ũ', 'ũ')\n",
        "  text = text.replace('Ụ', 'ụ')\n",
        "\n",
        "  text = text.replace('Ư', 'ư')\n",
        "  text = text.replace('Ừ', 'ừ')\n",
        "  text = text.replace('Ứ', 'ứ')\n",
        "  text = text.replace('Ử', 'ử')\n",
        "  text = text.replace('Ữ', 'ữ')\n",
        "  text = text.replace('Ự', 'ự')\n",
        "\n",
        "  text = text.replace('O', 'o')\n",
        "  text = text.replace('Ò', 'ò')\n",
        "  text = text.replace('Ó', 'ó')\n",
        "  text = text.replace('Ỏ', 'ỏ')\n",
        "  text = text.replace('Õ', 'õ')\n",
        "  text = text.replace('Ọ', 'ọ')\n",
        "  \n",
        "  text = text.replace('Ơ', 'ơ')\n",
        "  text = text.replace('Ờ', 'ờ')\n",
        "  text = text.replace('Ớ', 'ớ')\n",
        "  text = text.replace('Ở', 'ở')\n",
        "  text = text.replace('Ỡ', 'ỡ')\n",
        "  text = text.replace('Ợ', 'ợ')\n",
        "  \n",
        "  text = text.replace('Ô', 'ô')\n",
        "  text = text.replace('Ồ', 'ồ')\n",
        "  text = text.replace('Ố', 'ố')\n",
        "  text = text.replace('Ổ', 'ổ')\n",
        "  text = text.replace('Ỗ', 'ỗ')\n",
        "  text = text.replace('Ộ', 'ộ')\n",
        "\n",
        "  # Some other simple standardization.\n",
        "  text = text.replace('ko ', 'không ')\n",
        "  text = text.replace('dc ', 'được ')\n",
        "  text = text.lower()\n",
        "  return text + '\\n'\n",
        "  \n",
        "\n",
        "# Write the result to a text file:\n",
        "train_text_file = os.path.join(foody_data_dir, 'foody_processed_train.txt')\n",
        "review_lens = []  # number of lines in each review.\n",
        "with open(train_text_file, 'w') as f:\n",
        "  for text in all_texts:\n",
        "    processed_text = back_translate_preprocess(text)\n",
        "    f.write(processed_text + '\\n')\n",
        "    num_lines = processed_text.count('\\n')\n",
        "    review_lens.append(num_lines)\n",
        "    \n",
        "    \n",
        "!head -n 20 $train_text_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDtq4U1OKvBh",
        "colab_type": "text"
      },
      "source": [
        "# Pull or Clone code from our VietAI's Github `vietai/dab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNnFxOojKuc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = '/content/dab'\n",
        "if not os.path.exists(src):\n",
        "  !git clone https://github.com/vietai/dab.git\n",
        "else:\n",
        "  % cd $src\n",
        "  !git pull\n",
        "  % cd /\n",
        "  \n",
        "!ls $src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRbab_udK2UQ",
        "colab_type": "text"
      },
      "source": [
        "# Back-translate\n",
        "\n",
        "You can surely perform beam search decoding one sentence after another until finish. This, however, would take a very long time, even with this dataset of only 27K examples. Our code helps you with handling all possible speedups you can make:\n",
        "\n",
        "* Decoding on ML accelerator hardware (GPU or TPU).\n",
        "* Batch processing of sentences.\n",
        "* Processing sentences of similar length at the same time.\n",
        "* Saving intermediate results during the process to deal with Colab Notebook preemtions.\n",
        "\n",
        "At its core, the `back_translate.py` script essentially packages two calls to `t2t-decode` from vi -> en and then en -> vi. We added the ability to save intermediate results after every single batch of data since back-translating the full dataset can take quite a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngordrxKCWk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "output_file = os.path.join(foody_data_dir, 'foody_paraphrased_train.txt')\n",
        "\n",
        "is_demo = True  #@param {type:\"boolean\"}\n",
        "from_ckpt = '/content/vien-translation/checkpoints/translate_vien_iwslt32k_tiny/avg/'  # @param{type: \"string\"}\n",
        "to_ckpt = '/content/vien-translation/checkpoints/translate_envi_iwslt32k_tiny/avg/'  # @param{type: \"string\"}\n",
        "\n",
        "\n",
        "if is_demo:\n",
        "  paraphrase_from_file = os.path.join(foody_data_dir, 'demo_train.txt')\n",
        "  paraphrase_to_file = os.path.join(foody_data_dir, 'demo_out.txt')\n",
        "  # Remove previous results to run again.\n",
        "  tmp_file = paraphrase_from_file + '.tmp.en.txt'\n",
        "  if tf.gfile.Exists(tmp_file):\n",
        "    tf.gfile.Remove(tmp_file)\n",
        "  if tf.gfile.Exists(paraphrase_to_file):\n",
        "    tf.gfile.Remove(paraphrase_to_file)\n",
        "else:\n",
        "  paraphrase_from_file = train_text_file\n",
        "  paraphrase_to_file = output_file\n",
        "\n",
        "\n",
        "# Without specifying batch_size in decode_hparams, the default value will be 32.\n",
        "!python $src/back_translate.py \\\n",
        "--model=transformer --hparams_set=transformer_tiny \\\n",
        "--decode_hparams=\"beam_size=4,alpha=0.6\" \\\n",
        "--paraphrase_from_file=$paraphrase_from_file \\\n",
        "--paraphrase_to_file=$paraphrase_to_file \\\n",
        "--from_ckpt=$from_ckpt \\\n",
        "--to_ckpt=$to_ckpt \\\n",
        "--output_dir=/dummy/ \\\n",
        "--use_tpu=$use_tpu \\\n",
        "--cloud_tpu_name=$tpu_address"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxcuG9BiTFf2",
        "colab_type": "text"
      },
      "source": [
        "# Print some paraphrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2b9BnCNCgE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 $train_text_file\n",
        "print('==' * 10)\n",
        "!head -n 10 $output_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwRr-UFT54H",
        "colab_type": "text"
      },
      "source": [
        "# Convert back-translated text to CSV format.\n",
        "\n",
        "First, we have to process `output_file` back into a list of reviews. Since each review can have multiple lines of text, we make use of `review_lens` recorded previously to group the lines in `output_file` back into individual reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVs1VYXT5oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we read the text in output_file\n",
        "with open(output_file, 'r') as f:\n",
        "  output_lines = f.readlines()\n",
        "\n",
        "# Now we group lines into individual reviews according to review_lens.\n",
        "bt_reviews = []  # bt == 'back translated'\n",
        "for review_len in review_lens:\n",
        "  review = output_lines[:review_len]\n",
        "  output_lines = output_lines[review_len+1:]\n",
        "  # combine the lines.\n",
        "  review = ''.join(review)\n",
        "  bt_reviews.append(review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1ctHwQWqcC",
        "colab_type": "text"
      },
      "source": [
        "Next, we put the list of reviews `bt_reviews` back into Pandas data frame and save it back to csv format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D0GnO8mVqwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bt_train_df = train_df.copy()   # bt == back translated\n",
        "bt_train_df['text'] = bt_reviews\n",
        "\n",
        "bt_train_csv = os.path.join(foody_data_dir, 'bt_train.csv')\n",
        "bt_train_df.to_csv(bt_train_csv, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGLiK4_kX1LC",
        "colab_type": "text"
      },
      "source": [
        "Done! Let's load the back-translated dataset and check to see if everything looks okay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTn5HOPnX04x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bt_train_df = pd.read_csv(bt_train_csv)\n",
        "bt_train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR96nDcTx5UD",
        "colab_type": "text"
      },
      "source": [
        "# Test Time Augmentation (TTA).\n",
        "\n",
        "If we have access to the back-translation models (vi->en & en->vi), can we make use of them during test time?\n",
        "\n",
        "The answer is yes, one idea called \"Test Time Augmentation\" asks the model to average its predictions on real and synthetic examples, generated from the real ones. This technique is employed widely in competitions and reliably gives improvement on Computer Vision benchmarks.\n",
        "\n",
        "To use TTA, here we simply repeat the above steps on the test data. Note that the release test data here does not have the `class` column so we are not cheating for using TTA. We have generated the back-translated test data in a separate colab using the same procedure presented so far. The result can be found in the same Cloud Storage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmmh8s_E1_CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bt_test_csv = os.path.join(foody_data_dir, 'bt_test.csv')\n",
        "bt_test_df = pd.read_csv(bt_test_csv)\n",
        "bt_test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7diP-HNP3w",
        "colab_type": "text"
      },
      "source": [
        "# Now let's train a model on the augmented data.\n",
        "\n",
        "Using Machine Learning to solve this dataset is a standard assignment in our VietAI's foundation class. We use methods/solution/code from this assignment, prepared by VietAI's teaching staff and is available to any student of the class. \n",
        "\n",
        "At the moment this material is not readily viewable to the public, but we are working towards that! For now we report the results obtained from training the solution model on the original dataset and the augmented dataset (column \"with BT\") here. We did not make any extra modification from the original solution and still got improvement right away! Here we report the results for two settings: (1) limited data where only 10K training examples is available, and (2) when there is a full access to 27K training examples.\n",
        "\n",
        "\n",
        "|     | Normal  | with BT| with TTA |\n",
        "|-----|---------|-----------------------|------------------------|\n",
        "| 27k | 85.86% | 86.05%               | __86.95%__                |\n",
        "| 10k | 83.48% | __85.91%__               | 85.05%                |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yvhttVKTkZu",
        "colab_type": "text"
      },
      "source": [
        "# Acknowledgements\n",
        "\n",
        "This work is made possible by [VietAI](http://vietai.org/). Special thanks to [Thang Luong](http://thangluong.com), Le Cao Thang, and Hoang Quy Phat for collaborating and giving comments.\n",
        "\n",
        "# References\n",
        "\n",
        "1. Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems. 2017.\n",
        "\n",
        "2. Vaswani, Ashish, et al. \"Tensor2tensor for neural machine translation.\" arXiv preprint arXiv.\n",
        "\n",
        "3. Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Improving neural machine translation models with monolingual data.\" arXiv preprint arXiv:1511.06709 (2015).\n",
        "\n",
        "4. Edunov, Sergey, et al. \"Understanding back-translation at scale.\" arXiv preprint arXiv:1808.09381 (2018)."
      ]
    }
  ]
}